{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GloVe\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEmailClassifier\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, emb_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, glove_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6B\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "class EmailClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, emb_dim=300, dropout=0.2, glove_version='6B'):\n",
    "        super(EmailClassifier, self).__init__()\n",
    "        self.glove = GloVe(name='6B', dim=emb_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.glove.vectors, freeze=True)\n",
    "        self.subject_encoder = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.subject_attention = Attention(hidden_dim*2)\n",
    "        self.body_encoder = nn.LSTM(emb_dim, *hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.body_attention = Attention(hidden_dim*2)\n",
    "        self.shared_dense = nn.Sequential(\n",
    "                            nn.Linear(hidden_dim*4, hidden_dim),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Dropout(dropout),)\n",
    "        self.category_classifier = nn.Linear(hidden_dim, 3)\n",
    "        self.sensitivity_classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, subject, body):\n",
    "        subject_emb = self.embedding(subject)\n",
    "        body_emb = self.embedding(body)\n",
    "        \n",
    "        subject_output, _ = self.subject_encoder(subject_emb)\n",
    "        subject_encoded = self.subject_attention(subject_output)\n",
    "        \n",
    "        body_output, _ = self.body_encoder(body_emb)\n",
    "        body_encoded = self.body_attention(body_output)\n",
    "        \n",
    "        combined = torch.cat([subject_encoded, body_encoded], dim=1)\n",
    "        shared_output = self.shared_dense(combined)\n",
    "        \n",
    "        category_logits = self.category_classifier(shared_output)\n",
    "        sensitivity_logits = self.sensitivity_classifier(shared_output)\n",
    "        \n",
    "        return category_logits, sensitivity_logits\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        attention_weights = F.softmax(self.attention(encoder_outputs), dim=1)\n",
    "        attention = torch.sum(attention_weights*encoder_outputs, dim=1)\n",
    "        return attention\n",
    "\n",
    "class Sensitivity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sensitivity, self).__init__()\n",
    "\n",
    "    def forward(self, category_probs, acad_sensitivity):\n",
    "        student_mask = category_probs[:, 0].unsqueeze(1)\n",
    "        corp_mask = category_probs[:, 1].unsqueeze(1)\n",
    "        acad_mask = category_probs[:, 2].unsqueeze(1)\n",
    "        sensitivity = corp_mask + acad_mask*acad_sensitivity\n",
    "        return sensitivity\n",
    "\n",
    "def loss(category_logits, sensitivity_logits, category_labels, sensitivity_labels, alpha):\n",
    "    category_loss = F.cross_entropy(category_logits, category_labels)\n",
    "    ac_mask = (category_labels == 2).float()\n",
    "    sensitivity_loss = F.binary_cross_entropy_with_logits(sensitivity_logits.squeeze(), sensitivity_labels, reduction='none')\n",
    "    sensitivity_loss = (sensitivity_loss * ac_mask).mean()\n",
    "    return alpha * category_loss + (1 - alpha) * sensitivity_loss\n",
    "\n",
    "def preprocess_text(text, max_length, glove):\n",
    "    tokens = text.lower().split()  # Simple tokenization\n",
    "    indices = [glove.stoi.get(token, glove.stoi['<unk>']) for token in tokens]\n",
    "    if len(indices) < max_length:\n",
    "        indices += [glove.stoi['<pad>']] * (max_length - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_length]\n",
    "    return torch.tensor(indices)    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
